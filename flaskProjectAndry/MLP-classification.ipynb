{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création d'un modèle de classification basé sur MLP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#importation des library\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn import datasets\n",
    "import flask\n",
    "from flask import Flask\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "digits = datasets.load_digits()\n",
    "images_and_labels = list(zip(digits.images, digits.target))\n",
    "for index, (image, label) in enumerate(images_and_labels[:4]):\n",
    "    plt.subplot(2, 4, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('Training: %i' % label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digits avec 1347 training - 450 test samples\n",
      "('distribution du dataset:', array([178, 182, 177, 183, 181, 182, 181, 179, 174, 180], dtype=int64))\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   12.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:   22.8s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.29916725\n",
      "Iteration 2, loss = 2.27600261\n",
      "Iteration 3, loss = 2.25554583\n",
      "Iteration 4, loss = 2.23371173\n",
      "Iteration 5, loss = 2.20908278\n",
      "Iteration 6, loss = 2.18160849\n",
      "Iteration 7, loss = 2.15090606\n",
      "Iteration 8, loss = 2.11577719\n",
      "Iteration 9, loss = 2.07669874\n",
      "Iteration 10, loss = 2.03251125\n",
      "Iteration 11, loss = 1.98427584\n",
      "Iteration 12, loss = 1.93117720\n",
      "Iteration 13, loss = 1.87382277\n",
      "Iteration 14, loss = 1.81267462\n",
      "Iteration 15, loss = 1.74874018\n",
      "Iteration 16, loss = 1.68286676\n",
      "Iteration 17, loss = 1.61557331\n",
      "Iteration 18, loss = 1.54753910\n",
      "Iteration 19, loss = 1.48095965\n",
      "Iteration 20, loss = 1.41430220\n",
      "Iteration 21, loss = 1.34905789\n",
      "Iteration 22, loss = 1.28678374\n",
      "Iteration 23, loss = 1.22650283\n",
      "Iteration 24, loss = 1.16865583\n",
      "Iteration 25, loss = 1.11382233\n",
      "Iteration 26, loss = 1.06185058\n",
      "Iteration 27, loss = 1.01326780\n",
      "Iteration 28, loss = 0.96770951\n",
      "Iteration 29, loss = 0.92349942\n",
      "Iteration 30, loss = 0.88402728\n",
      "Iteration 31, loss = 0.84579507\n",
      "Iteration 32, loss = 0.81096550\n",
      "Iteration 33, loss = 0.77769379\n",
      "Iteration 34, loss = 0.74654215\n",
      "Iteration 35, loss = 0.71730557\n",
      "Iteration 36, loss = 0.69005115\n",
      "Iteration 37, loss = 0.66468177\n",
      "Iteration 38, loss = 0.64050559\n",
      "Iteration 39, loss = 0.61788187\n",
      "Iteration 40, loss = 0.59708277\n",
      "Iteration 41, loss = 0.57660128\n",
      "Iteration 42, loss = 0.55797932\n",
      "Iteration 43, loss = 0.53981982\n",
      "Iteration 44, loss = 0.52345553\n",
      "Iteration 45, loss = 0.50736780\n",
      "Iteration 46, loss = 0.49222723\n",
      "Iteration 47, loss = 0.47803408\n",
      "Iteration 48, loss = 0.46449380\n",
      "Iteration 49, loss = 0.45198533\n",
      "Iteration 50, loss = 0.44012997\n",
      "Iteration 51, loss = 0.42763054\n",
      "Iteration 52, loss = 0.41589618\n",
      "Iteration 53, loss = 0.40541808\n",
      "Iteration 54, loss = 0.39554040\n",
      "Iteration 55, loss = 0.38588658\n",
      "Iteration 56, loss = 0.37666745\n",
      "Iteration 57, loss = 0.36774296\n",
      "Iteration 58, loss = 0.35909463\n",
      "Iteration 59, loss = 0.35135337\n",
      "Iteration 60, loss = 0.34391238\n",
      "Iteration 61, loss = 0.33627301\n",
      "Iteration 62, loss = 0.32874837\n",
      "Iteration 63, loss = 0.32217421\n",
      "Iteration 64, loss = 0.31556782\n",
      "Iteration 65, loss = 0.30913543\n",
      "Iteration 66, loss = 0.30288910\n",
      "Iteration 67, loss = 0.29736030\n",
      "Iteration 68, loss = 0.29199617\n",
      "Iteration 69, loss = 0.28626171\n",
      "Iteration 70, loss = 0.28101314\n",
      "Iteration 71, loss = 0.27560150\n",
      "Iteration 72, loss = 0.27143254\n",
      "Iteration 73, loss = 0.26615047\n",
      "Iteration 74, loss = 0.26143138\n",
      "Iteration 75, loss = 0.25737030\n",
      "Iteration 76, loss = 0.25305603\n",
      "Iteration 77, loss = 0.24911842\n",
      "Iteration 78, loss = 0.24509452\n",
      "Iteration 79, loss = 0.24108229\n",
      "Iteration 80, loss = 0.23732889\n",
      "Iteration 81, loss = 0.23372571\n",
      "Iteration 82, loss = 0.23012507\n",
      "Iteration 83, loss = 0.22696463\n",
      "Iteration 84, loss = 0.22345650\n",
      "Iteration 85, loss = 0.22025921\n",
      "Iteration 86, loss = 0.21708966\n",
      "Iteration 87, loss = 0.21400668\n",
      "Iteration 88, loss = 0.21098449\n",
      "Iteration 89, loss = 0.20823894\n",
      "Iteration 90, loss = 0.20548677\n",
      "Iteration 91, loss = 0.20268345\n",
      "Iteration 92, loss = 0.20002401\n",
      "Iteration 93, loss = 0.19747957\n",
      "Iteration 94, loss = 0.19508163\n",
      "Iteration 95, loss = 0.19249009\n",
      "Iteration 96, loss = 0.19011150\n",
      "Iteration 97, loss = 0.18774759\n",
      "Iteration 98, loss = 0.18585332\n",
      "Iteration 99, loss = 0.18337177\n",
      "Iteration 100, loss = 0.18119477\n",
      "Iteration 101, loss = 0.17872183\n",
      "Iteration 102, loss = 0.17669649\n",
      "Iteration 103, loss = 0.17495636\n",
      "Iteration 104, loss = 0.17273298\n",
      "Iteration 105, loss = 0.17139976\n",
      "Iteration 106, loss = 0.16904499\n",
      "Iteration 107, loss = 0.16729940\n",
      "Iteration 108, loss = 0.16537864\n",
      "Iteration 109, loss = 0.16329807\n",
      "Iteration 110, loss = 0.16167344\n",
      "Iteration 111, loss = 0.16024434\n",
      "Iteration 112, loss = 0.15822855\n",
      "Iteration 113, loss = 0.15672183\n",
      "Iteration 114, loss = 0.15513917\n",
      "Iteration 115, loss = 0.15359148\n",
      "Iteration 116, loss = 0.15190002\n",
      "Iteration 117, loss = 0.15045403\n",
      "Iteration 118, loss = 0.14909071\n",
      "Iteration 119, loss = 0.14737825\n",
      "Iteration 120, loss = 0.14605807\n",
      "Iteration 121, loss = 0.14458488\n",
      "Iteration 122, loss = 0.14368107\n",
      "Iteration 123, loss = 0.14184344\n",
      "Iteration 124, loss = 0.14066662\n",
      "Iteration 125, loss = 0.13941267\n",
      "Iteration 126, loss = 0.13785195\n",
      "Iteration 127, loss = 0.13684299\n",
      "Iteration 128, loss = 0.13584673\n",
      "Iteration 129, loss = 0.13453512\n",
      "Iteration 130, loss = 0.13317119\n",
      "Iteration 131, loss = 0.13198688\n",
      "Iteration 132, loss = 0.13082527\n",
      "Iteration 133, loss = 0.12960486\n",
      "Iteration 134, loss = 0.12839766\n",
      "Iteration 135, loss = 0.12754327\n",
      "Iteration 136, loss = 0.12639614\n",
      "Iteration 137, loss = 0.12524442\n",
      "Iteration 138, loss = 0.12461606\n",
      "Iteration 139, loss = 0.12329314\n",
      "Iteration 140, loss = 0.12204251\n",
      "Iteration 141, loss = 0.12106705\n",
      "Iteration 142, loss = 0.12005667\n",
      "Iteration 143, loss = 0.11920618\n",
      "Iteration 144, loss = 0.11827375\n",
      "Iteration 145, loss = 0.11754734\n",
      "Iteration 146, loss = 0.11634450\n",
      "Iteration 147, loss = 0.11564319\n",
      "Iteration 148, loss = 0.11502113\n",
      "Iteration 149, loss = 0.11351487\n",
      "Iteration 150, loss = 0.11264410\n",
      "Iteration 151, loss = 0.11187252\n",
      "Iteration 152, loss = 0.11091604\n",
      "Iteration 153, loss = 0.11000306\n",
      "Iteration 154, loss = 0.10920311\n",
      "Iteration 155, loss = 0.10850451\n",
      "Iteration 156, loss = 0.10764752\n",
      "Iteration 157, loss = 0.10729645\n",
      "Iteration 158, loss = 0.10595384\n",
      "Iteration 159, loss = 0.10606044\n",
      "Iteration 160, loss = 0.10452506\n",
      "Iteration 161, loss = 0.10386649\n",
      "Iteration 162, loss = 0.10290209\n",
      "Iteration 163, loss = 0.10214972\n",
      "Iteration 164, loss = 0.10141463\n",
      "Iteration 165, loss = 0.10053019\n",
      "Iteration 166, loss = 0.09986707\n",
      "Iteration 167, loss = 0.09908080\n",
      "Iteration 168, loss = 0.09841237\n",
      "Iteration 169, loss = 0.09798340\n",
      "Iteration 170, loss = 0.09686662\n",
      "Iteration 171, loss = 0.09621038\n",
      "Iteration 172, loss = 0.09550011\n",
      "Iteration 173, loss = 0.09514815\n",
      "Iteration 174, loss = 0.09421908\n",
      "Iteration 175, loss = 0.09382294\n",
      "Iteration 176, loss = 0.09290112\n",
      "Iteration 177, loss = 0.09235642\n",
      "Iteration 178, loss = 0.09167673\n",
      "Iteration 179, loss = 0.09101524\n",
      "Iteration 180, loss = 0.09021447\n",
      "Iteration 181, loss = 0.08967103\n",
      "Iteration 182, loss = 0.08897702\n",
      "Iteration 183, loss = 0.08836741\n",
      "Iteration 184, loss = 0.08790686\n",
      "Iteration 185, loss = 0.08719832\n",
      "Iteration 186, loss = 0.08666878\n",
      "Iteration 187, loss = 0.08608857\n",
      "Iteration 188, loss = 0.08541020\n",
      "Iteration 189, loss = 0.08495635\n",
      "Iteration 190, loss = 0.08431973\n",
      "Iteration 191, loss = 0.08368267\n",
      "Iteration 192, loss = 0.08318523\n",
      "Iteration 193, loss = 0.08270432\n",
      "Iteration 194, loss = 0.08198620\n",
      "Iteration 195, loss = 0.08166212\n",
      "Iteration 196, loss = 0.08108137\n",
      "Iteration 197, loss = 0.08084976\n",
      "Iteration 198, loss = 0.07988535\n",
      "Iteration 199, loss = 0.07953389\n",
      "Iteration 200, loss = 0.07928643\n",
      "('Score best mean cross-validated:', 0.9643652561247216)\n",
      "('Meilleur parametre dans:', {'hidden_layer_sizes': (512,)})\n",
      "Modèle sauvegarder\n",
      "('Test accuracy:', 0.9733333333333334)\n",
      "0:02:25.369000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda2\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "start=dt.datetime.now()\n",
    "n_samples = len(digits.images)\n",
    "#X,y = digits.data, digits.target\n",
    "X,y = digits.images, digits.target\n",
    "X = X.reshape(-1, 64)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X / 255., y, test_size=0.25)\n",
    "print('digits avec %d training - %d test samples' % (len(y_train), len(y_test)))\n",
    "print('distribution du dataset:', np.bincount(y.astype('int64')))\n",
    "#training\n",
    "params = {'hidden_layer_sizes': [(256,), (512,), (128, 256, 128,)]}\n",
    "mlp = MLPClassifier(verbose=10, learning_rate='adaptive')\n",
    "clf = GridSearchCV(mlp, params, verbose=10, n_jobs=-1, cv=5)\n",
    "clf.fit(X_train, y_train)\n",
    "print('Score best mean cross-validated:', clf.best_score_)\n",
    "print('Meilleur parametre dans:', clf.best_params_)\n",
    "joblib.dump(clf, 'model_MLP.pkl')\n",
    "print(\"Modèle sauvegarder\")\n",
    "clf = clf.best_estimator_\n",
    "print('Test accuracy:', clf.score(X_test, y_test))\n",
    "print(dt.datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modele testable par appel de webservice (API) via Flask\n",
    "Etape 1 run Flask app dans un terminale prompt\n",
    "    >python app.py\n",
    "    \n",
    "Etape 2\n",
    "Une fois que app running, nous pouvons faire une requete pour une prediction\n",
    " * Running on http://127.0.0.1:8000/ (Press CTRL+C to quit)\n",
    " * Restarting with stat\n",
    " * Debugger is active!\n",
    " * Debugger pin code: 987-847-325"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#Export testing sets to csv file\n",
    "import pandas as pd\n",
    "\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "y_test_df = pd.DataFrame(y_test)\n",
    "X_test = X_test_df.to_csv('X_test.csv', sep=',',encoding='utf-8', index=False)\n",
    "y_test = y_test_df.to_csv('y_test.csv', sep=',',encoding='utf-8', index=False)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_t', (450L, 64L), 28800)\n",
      "('y_t', (450L, 1L), 450)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "X_test_df = pd.read_csv('X_test.csv')\n",
    "y_test_df = pd.read_csv('y_test.csv')\n",
    "X_t = np.array(X_test_df)\n",
    "y_t = np.array(y_test_df)\n",
    "print(\"X_t\", X_t.shape, X_t.size)\n",
    "print(\"y_t\", y_t.shape, y_t.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('y_pred', (450L,))\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.97      0.99        37\n",
      "          1       0.92      0.92      0.92        48\n",
      "          2       1.00      1.00      1.00        42\n",
      "          3       0.98      1.00      0.99        44\n",
      "          4       0.95      1.00      0.97        37\n",
      "          5       1.00      0.98      0.99        46\n",
      "          6       0.95      1.00      0.98        42\n",
      "          7       1.00      0.98      0.99        53\n",
      "          8       0.98      0.89      0.93        46\n",
      "          9       0.96      1.00      0.98        55\n",
      "\n",
      "avg / total       0.97      0.97      0.97       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_pred = clf.predict(X_t)\n",
    "print(\"y_pred\", y_pred.shape)\n",
    "print (classification_report(y_t, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

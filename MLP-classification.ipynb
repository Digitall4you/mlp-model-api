{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création d'un modèle de classification basé sur MLP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#importation des library\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn import datasets\n",
    "import flask\n",
    "from flask import Flask\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "digits = datasets.load_digits()\n",
    "images_and_labels = list(zip(digits.images, digits.target))\n",
    "for index, (image, label) in enumerate(images_and_labels[:4]):\n",
    "    plt.subplot(2, 4, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('Training: %i' % label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digits avec 1347 training - 450 test samples\n",
      "('distribution du dataset:', array([178, 182, 177, 183, 181, 182, 181, 179, 174, 180], dtype=int64))\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   13.5s\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:   26.5s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  2.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.30282612\n",
      "Iteration 2, loss = 2.27874822\n",
      "Iteration 3, loss = 2.25758746\n",
      "Iteration 4, loss = 2.23537552\n",
      "Iteration 5, loss = 2.21087020\n",
      "Iteration 6, loss = 2.18320658\n",
      "Iteration 7, loss = 2.15218200\n",
      "Iteration 8, loss = 2.11647911\n",
      "Iteration 9, loss = 2.07654452\n",
      "Iteration 10, loss = 2.03174766\n",
      "Iteration 11, loss = 1.98209648\n",
      "Iteration 12, loss = 1.92878101\n",
      "Iteration 13, loss = 1.86969594\n",
      "Iteration 14, loss = 1.80702622\n",
      "Iteration 15, loss = 1.74214012\n",
      "Iteration 16, loss = 1.67410798\n",
      "Iteration 17, loss = 1.60442195\n",
      "Iteration 18, loss = 1.53386981\n",
      "Iteration 19, loss = 1.46424518\n",
      "Iteration 20, loss = 1.39511649\n",
      "Iteration 21, loss = 1.32847408\n",
      "Iteration 22, loss = 1.26289910\n",
      "Iteration 23, loss = 1.20036272\n",
      "Iteration 24, loss = 1.14135178\n",
      "Iteration 25, loss = 1.08425193\n",
      "Iteration 26, loss = 1.03202526\n",
      "Iteration 27, loss = 0.98158695\n",
      "Iteration 28, loss = 0.93505376\n",
      "Iteration 29, loss = 0.89128197\n",
      "Iteration 30, loss = 0.85048273\n",
      "Iteration 31, loss = 0.81262029\n",
      "Iteration 32, loss = 0.77758379\n",
      "Iteration 33, loss = 0.74445530\n",
      "Iteration 34, loss = 0.71351529\n",
      "Iteration 35, loss = 0.68495476\n",
      "Iteration 36, loss = 0.65915809\n",
      "Iteration 37, loss = 0.63396742\n",
      "Iteration 38, loss = 0.61078607\n",
      "Iteration 39, loss = 0.58876759\n",
      "Iteration 40, loss = 0.56860389\n",
      "Iteration 41, loss = 0.54925541\n",
      "Iteration 42, loss = 0.53084516\n",
      "Iteration 43, loss = 0.51373479\n",
      "Iteration 44, loss = 0.49816889\n",
      "Iteration 45, loss = 0.48204398\n",
      "Iteration 46, loss = 0.46771013\n",
      "Iteration 47, loss = 0.45470719\n",
      "Iteration 48, loss = 0.44270051\n",
      "Iteration 49, loss = 0.42983872\n",
      "Iteration 50, loss = 0.41732251\n",
      "Iteration 51, loss = 0.40622679\n",
      "Iteration 52, loss = 0.39564705\n",
      "Iteration 53, loss = 0.38508072\n",
      "Iteration 54, loss = 0.37617306\n",
      "Iteration 55, loss = 0.36655236\n",
      "Iteration 56, loss = 0.35772370\n",
      "Iteration 57, loss = 0.34952141\n",
      "Iteration 58, loss = 0.34121934\n",
      "Iteration 59, loss = 0.33402294\n",
      "Iteration 60, loss = 0.32672687\n",
      "Iteration 61, loss = 0.31939404\n",
      "Iteration 62, loss = 0.31236509\n",
      "Iteration 63, loss = 0.30617129\n",
      "Iteration 64, loss = 0.29950396\n",
      "Iteration 65, loss = 0.29363821\n",
      "Iteration 66, loss = 0.28772304\n",
      "Iteration 67, loss = 0.28204279\n",
      "Iteration 68, loss = 0.27659722\n",
      "Iteration 69, loss = 0.27111091\n",
      "Iteration 70, loss = 0.26652882\n",
      "Iteration 71, loss = 0.26144116\n",
      "Iteration 72, loss = 0.25709444\n",
      "Iteration 73, loss = 0.25179369\n",
      "Iteration 74, loss = 0.24824097\n",
      "Iteration 75, loss = 0.24344046\n",
      "Iteration 76, loss = 0.23952729\n",
      "Iteration 77, loss = 0.23528352\n",
      "Iteration 78, loss = 0.23163984\n",
      "Iteration 79, loss = 0.22793085\n",
      "Iteration 80, loss = 0.22433848\n",
      "Iteration 81, loss = 0.22058671\n",
      "Iteration 82, loss = 0.21739599\n",
      "Iteration 83, loss = 0.21423451\n",
      "Iteration 84, loss = 0.21094608\n",
      "Iteration 85, loss = 0.20845629\n",
      "Iteration 86, loss = 0.20510227\n",
      "Iteration 87, loss = 0.20177956\n",
      "Iteration 88, loss = 0.19887445\n",
      "Iteration 89, loss = 0.19619750\n",
      "Iteration 90, loss = 0.19356096\n",
      "Iteration 91, loss = 0.19105272\n",
      "Iteration 92, loss = 0.18878238\n",
      "Iteration 93, loss = 0.18593025\n",
      "Iteration 94, loss = 0.18336523\n",
      "Iteration 95, loss = 0.18123255\n",
      "Iteration 96, loss = 0.17858076\n",
      "Iteration 97, loss = 0.17640636\n",
      "Iteration 98, loss = 0.17459040\n",
      "Iteration 99, loss = 0.17239832\n",
      "Iteration 100, loss = 0.17020939\n",
      "Iteration 101, loss = 0.16797196\n",
      "Iteration 102, loss = 0.16604677\n",
      "Iteration 103, loss = 0.16383153\n",
      "Iteration 104, loss = 0.16227123\n",
      "Iteration 105, loss = 0.16013775\n",
      "Iteration 106, loss = 0.15860355\n",
      "Iteration 107, loss = 0.15694146\n",
      "Iteration 108, loss = 0.15536218\n",
      "Iteration 109, loss = 0.15337692\n",
      "Iteration 110, loss = 0.15164828\n",
      "Iteration 111, loss = 0.15025774\n",
      "Iteration 112, loss = 0.14813766\n",
      "Iteration 113, loss = 0.14694725\n",
      "Iteration 114, loss = 0.14531865\n",
      "Iteration 115, loss = 0.14418773\n",
      "Iteration 116, loss = 0.14259338\n",
      "Iteration 117, loss = 0.14130076\n",
      "Iteration 118, loss = 0.13957775\n",
      "Iteration 119, loss = 0.13806566\n",
      "Iteration 120, loss = 0.13685812\n",
      "Iteration 121, loss = 0.13549813\n",
      "Iteration 122, loss = 0.13419192\n",
      "Iteration 123, loss = 0.13276068\n",
      "Iteration 124, loss = 0.13188144\n",
      "Iteration 125, loss = 0.13036874\n",
      "Iteration 126, loss = 0.12921442\n",
      "Iteration 127, loss = 0.12808679\n",
      "Iteration 128, loss = 0.12725472\n",
      "Iteration 129, loss = 0.12564995\n",
      "Iteration 130, loss = 0.12475147\n",
      "Iteration 131, loss = 0.12381688\n",
      "Iteration 132, loss = 0.12243259\n",
      "Iteration 133, loss = 0.12139888\n",
      "Iteration 134, loss = 0.12000482\n",
      "Iteration 135, loss = 0.11902572\n",
      "Iteration 136, loss = 0.11821450\n",
      "Iteration 137, loss = 0.11705950\n",
      "Iteration 138, loss = 0.11624949\n",
      "Iteration 139, loss = 0.11503635\n",
      "Iteration 140, loss = 0.11446400\n",
      "Iteration 141, loss = 0.11326901\n",
      "Iteration 142, loss = 0.11219952\n",
      "Iteration 143, loss = 0.11140947\n",
      "Iteration 144, loss = 0.11063598\n",
      "Iteration 145, loss = 0.10992280\n",
      "Iteration 146, loss = 0.10877750\n",
      "Iteration 147, loss = 0.10789952\n",
      "Iteration 148, loss = 0.10733375\n",
      "Iteration 149, loss = 0.10655455\n",
      "Iteration 150, loss = 0.10553981\n",
      "Iteration 151, loss = 0.10438367\n",
      "Iteration 152, loss = 0.10374332\n",
      "Iteration 153, loss = 0.10293737\n",
      "Iteration 154, loss = 0.10241699\n",
      "Iteration 155, loss = 0.10134638\n",
      "Iteration 156, loss = 0.10062297\n",
      "Iteration 157, loss = 0.09989650\n",
      "Iteration 158, loss = 0.09902945\n",
      "Iteration 159, loss = 0.09856994\n",
      "Iteration 160, loss = 0.09786834\n",
      "Iteration 161, loss = 0.09710527\n",
      "Iteration 162, loss = 0.09642523\n",
      "Iteration 163, loss = 0.09591994\n",
      "Iteration 164, loss = 0.09515711\n",
      "Iteration 165, loss = 0.09429387\n",
      "Iteration 166, loss = 0.09389619\n",
      "Iteration 167, loss = 0.09335807\n",
      "Iteration 168, loss = 0.09219655\n",
      "Iteration 169, loss = 0.09145816\n",
      "Iteration 170, loss = 0.09147443\n",
      "Iteration 171, loss = 0.09037455\n",
      "Iteration 172, loss = 0.09011632\n",
      "Iteration 173, loss = 0.08940176\n",
      "Iteration 174, loss = 0.08849162\n",
      "Iteration 175, loss = 0.08818338\n",
      "Iteration 176, loss = 0.08750728\n",
      "Iteration 177, loss = 0.08688527\n",
      "Iteration 178, loss = 0.08626818\n",
      "Iteration 179, loss = 0.08572926\n",
      "Iteration 180, loss = 0.08511690\n",
      "Iteration 181, loss = 0.08464162\n",
      "Iteration 182, loss = 0.08426991\n",
      "Iteration 183, loss = 0.08364888\n",
      "Iteration 184, loss = 0.08286819\n",
      "Iteration 185, loss = 0.08273958\n",
      "Iteration 186, loss = 0.08195460\n",
      "Iteration 187, loss = 0.08156906\n",
      "Iteration 188, loss = 0.08090682\n",
      "Iteration 189, loss = 0.08026752\n",
      "Iteration 190, loss = 0.07979595\n",
      "Iteration 191, loss = 0.07930396\n",
      "Iteration 192, loss = 0.07915083\n",
      "Iteration 193, loss = 0.07862845\n",
      "Iteration 194, loss = 0.07780302\n",
      "Iteration 195, loss = 0.07751053\n",
      "Iteration 196, loss = 0.07700045\n",
      "Iteration 197, loss = 0.07650511\n",
      "Iteration 198, loss = 0.07588710\n",
      "Iteration 199, loss = 0.07591115\n",
      "Iteration 200, loss = 0.07509439\n",
      "('Score best mean cross-validated:', 0.9658500371195249)\n",
      "('Meilleur parametre dans:', {'hidden_layer_sizes': (512,)})\n",
      "Modèle sauvegarder\n",
      "('Test accuracy:', 0.96)\n",
      "0:03:07.210000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andry\\Anaconda2\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "start=dt.datetime.now()\n",
    "n_samples = len(digits.images)\n",
    "X,y = digits.data, digits.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X / 255., y, test_size=0.25)\n",
    "print('digits avec %d training - %d test samples' % (len(y_train), len(y_test)))\n",
    "print('distribution du dataset:', np.bincount(y.astype('int64')))\n",
    "#training\n",
    "params = {'hidden_layer_sizes': [(256,), (512,), (128, 256, 128,)]}\n",
    "mlp = MLPClassifier(verbose=10, learning_rate='adaptive')\n",
    "clf = GridSearchCV(mlp, params, verbose=10, n_jobs=-1, cv=5)\n",
    "clf.fit(X_train, y_train)\n",
    "print('Score best mean cross-validated:', clf.best_score_)\n",
    "print('Meilleur parametre dans:', clf.best_params_)\n",
    "joblib.dump(clf, 'model_MLP.pkl')\n",
    "print(\"Modèle sauvegarder\")\n",
    "clf = clf.best_estimator_\n",
    "print('Test accuracy:', clf.score(X_test, y_test))\n",
    "print(dt.datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modele testable par appel de webservice (API) via Flask\n",
    "Etape 1 run Flask app dans un terminale prompt\n",
    "    >python app.py\n",
    "    \n",
    "Etape 2\n",
    "Une fois que app running, nous pouvons faire une requete pour une prediction\n",
    " * Running on http://127.0.0.1:9000/ (Press CTRL+C to quit)\n",
    " * Restarting with stat\n",
    " * Debugger is active!\n",
    " * Debugger pin code: 580-987-602"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appel et requete du model\n",
    "url = \"http://127.0.0.1:9000/predict_api\"\n",
    "data = json.dumps({'X':X_test[0]})\n",
    "r = requests.post(url,data)\n",
    "print r.json()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
